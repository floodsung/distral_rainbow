{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from anyrl.algos import DQN\n",
    "from anyrl.envs import BatchedGymEnv\n",
    "from anyrl.envs.wrappers import BatchedFrameStack\n",
    "from anyrl.models import rainbow_models\n",
    "from anyrl.rollouts import BasicPlayer, PrioritizedReplayBuffer, NStepPlayer\n",
    "from anyrl.spaces import gym_space_vectorizer\n",
    "\n",
    "from sonic_util import AllowBacktracking, make_env\n",
    "import numpy as np\n",
    "import csv\n",
    "import ray\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=4,num_gpus=1)\n",
    "class MultiAgent():\n",
    "    \"\"\"docstring for MultiAgent\"\"\"\n",
    "    def __init__(self, num_agent=4):\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True \n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        agents = [DistralAgent(sess,i) for i in range(num_agent)]\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        [agent.update_target() for agent in agents]\n",
    "\n",
    "    def train(self,distill_policy_weights):\n",
    "\n",
    "        distill_grads_list = [agent.train(distill_policy_weights) for agent in agents]\n",
    "\n",
    "        return distill_grads_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)\n",
    "class SonicEnv():\n",
    "\n",
    "    def __init__(self,env_index):\n",
    "        train_file = csv.reader(open('./sonic-train.csv','r'),delimiter=',')\n",
    "        self.games = []\n",
    "        for i,row in enumerate(train_file):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            self.games.append(row)\n",
    "\n",
    "        self.env = AllowBacktracking(make_env(game=self.games[env_index][0],state=self.games[env_index][1]))\n",
    "\n",
    "    def step(self,action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def action_space(self):\n",
    "        return self.env.action_space.n\n",
    "\n",
    "    def observation_space(self):\n",
    "        return self.env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DistralAgent():\n",
    "\n",
    "    def __init__(self,sess,env_index):\n",
    "        # step 1: init env\n",
    "        self.env_index = env_index\n",
    "        self.env = SonicEnv.remote(env_index)\n",
    "        action_space = ray.get(self.env.action_space.remote())\n",
    "        observation_space = ray.get(self.env.observation_space.remote())\n",
    "  \n",
    "        self.sess = sess\n",
    "        with tf.Graph().as_default():\n",
    "            self.dqn = DQN(*rainbow_models(self.sess,\n",
    "                                          action_space,\n",
    "                                          gym_space_vectorizer(observation_space),\n",
    "                                          min_val=-200,\n",
    "                                          max_val=200))\n",
    "        self.player = NStepPlayer(BasicPlayer(self.env, self.dqn.online_net), 3)\n",
    "\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(500000, 0.5, 0.4, epsilon=0.1)\n",
    "        #self.sess.run(self.dqn.update_target)\n",
    "        self.steps_taken = 0\n",
    "        self.train_interval=1\n",
    "        self.target_interval=8192\n",
    "        self.batch_size=32\n",
    "        self.min_buffer_size=200\n",
    "        self.handle_ep=lambda steps, rew: None\n",
    "        self.next_target_update = self.target_interval\n",
    "        self.next_train_step = self.train_interval\n",
    "\n",
    "    def update_target(self):\n",
    "        self.sess.run(self.dqn.update_target)\n",
    "\n",
    "\n",
    "    def init_env(self,env_index):\n",
    "        train_file = csv.reader(open('./sonic-train.csv','r'),delimiter=',')\n",
    "        self.games = []\n",
    "        for i,row in enumerate(train_file):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            self.games.append(row)\n",
    "\n",
    "        env = AllowBacktracking(make_env(game=self.games[env_index][0],state=self.games[env_index][1]))\n",
    "\n",
    "        env = BatchedFrameStack(BatchedGymEnv([[env]]), num_images=4, concat=False)\n",
    "        return env\n",
    "\n",
    "    def train(self,distill_policy_weights):\n",
    "\n",
    "        self.dqn.set_distill_policy_weights(distill_policy_weights)\n",
    "\n",
    "        transitions = self.player.play()\n",
    "        distill_grads = 0\n",
    "        for trans in transitions:\n",
    "                if trans['is_last']:\n",
    "                    self.handle_ep(trans['episode_step'] + 1, trans['total_reward'])\n",
    "                self.replay_buffer.add_sample(trans)\n",
    "                self.steps_taken += 1\n",
    "                if self.replay_buffer.size >= self.min_buffer_size and self.steps_taken >= self.next_train_step:\n",
    "                    self.next_train_step = self.steps_taken + self.train_interval\n",
    "                    batch = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "                    grad_names = []\n",
    "                    for grad in self.dqn.distill_grads:\n",
    "                        if grad[0] != None:\n",
    "                            grad_names.append(grad[0])\n",
    "\n",
    "                    _,losses,distill_grads = self.sess.run((self.dqn.optim,self.dqn.losses,grad_names),\n",
    "                                         feed_dict=self.dqn.feed_dict(batch))\n",
    "                    self.replay_buffer.update_weights(batch, losses)\n",
    "\n",
    "                if self.steps_taken >= self.next_target_update:\n",
    "                    self.next_target_update = self.steps_taken + self.target_interval\n",
    "                    self.sess.run(self.dqn.update_target)\n",
    "\n",
    "        return distill_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/raylogs/.\n",
      "Waiting for redis server at 127.0.0.1:24205 to respond...\n"
     ]
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Actors cannot be created before ray.init() has been called.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-14eff8f1aeb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMultiAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-14eff8f1aeb8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMultiAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ray/actor.py\u001b[0m in \u001b[0;36mremote\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 raise Exception(\"Actors cannot be created before ray.init() \"\n\u001b[0m\u001b[1;32m    760\u001b[0m                                 \"has been called.\")\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Actors cannot be created before ray.init() has been called."
     ]
    }
   ],
   "source": [
    "agents = [MultiAgent.remote() for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
